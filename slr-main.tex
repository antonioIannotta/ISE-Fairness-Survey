\documentclass{article}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{algorithm2e}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{caption}
\usepackage{float}

\geometry{a4paper, left=25mm, right=25mm, top=30mm, bottom=30mm}

\title{Systematic Scientific Review on Fairness}
\author{Antonio Iannotta}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
    Ethical, fairness, and responsible AI have emerged as pivotal concerns in the design and construction of artificial intelligence systems. This systematic literature review aims to comprehensively analyze the evolving landscape of ethical and responsible AI practices and principles within the domain of AI system development. Over the past decade, the rapid proliferation of AI technologies has underscored the critical need for ethical considerations that extend beyond mere compliance. \\
    The review investigates the frameworks, methodologies, and best practices that advocate for embedding ethical, fairness, and responsible AI principles into the very fabric of AI systems during their design and construction. By examining a wide array of scholarly contributions, this review seeks to elucidate the key ethical challenges faced by AI developers, the strategies employed to integrate fairness and ethical considerations, and the emergent best practices for cultivating responsible AI. \\
    Furthermore, it will explore the impact of ethical and responsible AI design on society, policy, and the broader AI landscape. The findings from this review will provide valuable insights for AI practitioners, researchers, and policymakers, highlighting the importance of ethical and responsible AI as an inherent part of the AI design process.
\end{abstract}
\newpage
\section{Introduction}
Artificial Intelligence (AI) has witnessed remarkable advancements and an unprecedented integration into various aspects of our lives over the past decade. As AI systems become more pervasive, the ethical, fairness, and responsible design of these systems has become an increasingly critical concern. The development of AI is no longer solely about technological innovation and performance optimization but also encompasses a profound sense of social responsibility and ethical awareness. \\
This Systematic Literature Review (SLR) delves into the multifaceted realm of Ethical, Fairness, and Responsible AI by design/construction. The rapid evolution of AI technologies has brought about a shift in the AI paradigm, emphasizing the need for ethical and responsible AI from the outset, during the design and construction phases. It is no longer sufficient to address ethical and fairness considerations as an afterthought or as a post-hoc addition; instead, these principles must be deeply ingrained in the very core of AI systems. \\
The scope of this SLR is to provide a comprehensive analysis of the existing literature, frameworks, methodologies, and best practices surrounding the integration of ethical and responsible AI principles during the design and construction of AI systems. It explores the challenges faced by AI developers, the strategies employed to incorporate fairness and ethics, and the emergent best practices that advocate for responsible AI development. \\
Moreover, the impact of ethical and responsible AI design on society, policy, and the broader AI landscape is a key aspect of this review. As AI systems exert profound influence on various domains, including healthcare, finance, education, and beyond, the ethical and responsible use of AI becomes pivotal for ensuring equitable access and decision-making. \\
Through the synthesis of a diverse body of scholarly contributions, this SLR aspires to elucidate the evolving landscape of Ethical, Fairness, and Responsible AI by design/construction. By synthesizing the knowledge and insights derived from existing research, this review intends to provide valuable guidance to AI practitioners, researchers, and policymakers. The ultimate goal is to underscore the significance of ethical and responsible AI as an integral part of the AI design process, furthering our collective efforts in creating AI systems that prioritize societal well-being and fairness.
\newpage
\section{Methodology}
    \subsection{Search Strategy}
    \textbf{Database Searches:}
\begin{itemize}
    \item ACM Digital Library
    \item Google Scholar
\end{itemize}

\textbf{Search Strings:}
\begin{enumerate}
    \item "AI by design"
    \item "Fair-by-design algorithm"
    \item "Responsible AI by design"
    \item "Fair AI by construction"
    \item "Ethical AI by design"
\end{enumerate}

\textbf{Inclusion and Exclusion Criteria:}
\begin{itemize}
    \item Publication Period: Last 10 years (from 2013 to 2023).
    \item Language: English.
    \item Document Types: Scientific articles, conference papers, conference proceedings, and relevant academic documents.
    \item Excluded: Duplicate documents and articles not relevant to the research topic.
\end{itemize}

This search strategy is designed to retrieve relevant articles published in the last 10 years that address approaches for mitigating bias and ensuring fairness in artificial intelligence systems. Make sure to specify the start and end dates for the publication period. You can use these search strings in the specified databases to identify pertinent sources for your Systematic Literature Review.

    \subsection{Study Selection}
    In this section, we describe the process of study selection, which involved the identification and screening of relevant articles. The goal was to select articles that addressed bias mitigation approaches in the context of specific applications.

    \begin{enumerate}
        \item \textbf{Total Number of Identified Articles:} 10
        \item \textbf{Number of Articles Excluded after Initial Screening:} 5
        
        During the initial screening, 3 articles were excluded due to their strong dependency on the specific application context in which the proposed approaches were intended to be applied.
        
        \item \textbf{Number of Articles Excluded after Secondary Screening:} 1
        
        In the secondary screening, 1 article was excluded due to its deep specificity in a security-by-Design approach.
        
        \item \textbf{Screening Procedures:}
        
        \begin{itemize}
            \item \textbf{Initial Screening:} The initial screening focused on identifying articles that were not closely related to a specific application context. Articles with approaches highly dependent on the application context were excluded.
            
            \item \textbf{Secondary Screening:} During the secondary screening, the goal was to identify articles with a similar methodological approach.
        \end{itemize}
        
        \item \textbf{Exclusion Reasons:}
        
        Articles were excluded if the proposed algorithms and approaches were deemed highly context-specific, tailored to the particular application context presented by the algorithm.
    
    \end{enumerate}
    
    The study selection process ensured that the articles included in the review were relevant to the context of bias mitigation in specific applications, while those heavily dependent on context were excluded.
    \subsection{Data Extraction}
    \begin{itemize}
        \item \textbf{Responsible AI by Design in Practice}
        \begin{itemize}
            \item Title: \textbf{Responsible AI by Design in Practice}
            \item Authors: 
            \begin{itemize}
                \item Richard Benjamins
                \item Alberto Barbado
                \item Daniel Sierra
            \end{itemize}
            \item Year of Publication: 2019
            \item Abstract:
            \begin{abstract}
                Recently, a lot of attention has been given to undesired con-
                sequences of Artificial Intelligence (AI), such as unfair bias 
                leading to discrimination, or the lack of explanations of the 
                results of AI systems. There are several important questions 
                to  answer  before  AI  can  be  deployed  at  scale  in  our  busi-
                nesses  and  societies.  Most  of  these  issues  are  being  dis-
                cussed  by  experts  and the  wider communities,  and  it  seems 
                there is broad consensus on where they come from. There is, 
                however,  less  consensus  on,  and  experience  with  how  to 
                practically  deal  with  those  issues  in  organizations  that  de-
                velop  and  use  AI,  both  from  a  technical  and  organizational 
                perspective. In this paper, we discuss the practical case of a 
                large  organization  that  is  putting  in  place  a  company-wide 
                methodology to minimize the risk of undesired consequenc-
                es  of  AI.  We  hope  that  other  organizations  can  learn  from 
                this  and  that  our  experience  contributes  to  making  the  best 
                of AI while minimizing its risks.
            \end{abstract}
        \end{itemize}
        
        
        \item \textbf{Bridging the Gap Between AI and 
        Explainability in the GDPR: Towards 
        Trustworthiness-by-Design in 
        Automated Decision-Making}
        \begin{itemize}
            \item Title: \textbf{Bridging the Gap Between AI and 
            Explainability in the GDPR: Towards 
            Trustworthiness-by-Design in 
            Automated Decision-Making}
            \item Authors:
            \begin{itemize}
                \item Ronan Hamon
                \item Henrik Junklewitz
                \item Ignacio Sanchez
                \item Gianclaudio Malgieri
                \item Paul De Hert
            \end{itemize}
            \item Year of Publication: 2022
            \item Abstract:
            \begin{abstract}
                Can satisfactory explanations for complex machine learning models be achieved in high-risk automated decision-making? How can such explanations be integrated into a data protection framework safeguarding a right to explanation? This article explores from an interdisciplinary point of view the connection between existing legal requirements for the explainability of AI systems set out in the General Data Protection Regulation (GDPR) and the current state of the art in the field of explainable AI. It studies the challenges of providing human-legible explanations for current and future AI-based decision-making systems in practice, based on two scenarios of automated decision-making in credit scoring risks and medical diagnosis of COVID-19. These scenarios exemplify the trend towards increasingly complex machine learning algorithms in automated decision-making, both in terms of data and models. Current machine learning techniques, in particular those based on deep learning, are unable to make clear causal links between input data and final decisions. This represents a limitation for providing exact, human-legible reasons behind specific decisions and presents a serious challenge to the provision of satisfactory, fair, and transparent explanations. Therefore, the conclusion is that the quality of explanations might not be considered as an adequate safeguard for automated decision-making processes under the GDPR. Accordingly, additional tools should be considered to complement explanations. These could include algorithmic impact assessments, other forms of algorithmic justifications based on broader AI principles, and new technical developments in trustworthy AI. This suggests that eventually all of these approaches would need to be considered as a whole.
            \end{abstract}
        \end{itemize}
        
        \item \textbf{Artificial intelligence ethics by design.
        Evaluating public perception on the
        importance of ethical design principles
        of artificial intelligence}
        \begin{itemize}
            \item Title: \textbf{Artificial intelligence ethics by design.
            Evaluating public perception on the
            importance of ethical design principles
            of artificial intelligence}
            \item Authors:
            \begin{itemize}
                \item Kimon Kieslich
                \item Birte Keller
                \item Christopher Starke
            \end{itemize}
            \item Year of Publication: 2022
            \item Abstract:
            \begin{abstract}
                Despite the immense societal importance of ethically designing artificial intelligence, little research on the public percep-
                tions of ethical artificial intelligence principles exists. This becomes even more striking when considering that ethical arti-
                ficial intelligence development has the aim to be human-centric and of benefit for the whole society. In this study, we
                investigate how ethical principles (explainability, fairness, security, accountability, accuracy, privacy, and machine auton-
                omy) are weighted in comparison to each other. This is especially important, since simultaneously considering ethical
                principles is not only costly, but sometimes even impossible, as developers must make specific trade-off decisions. In
                this paper, we give first answers on the relative importance of ethical principles given a specific use case—the use of arti-
                ficial intelligence in tax fraud detection. The results of a large conjoint survey (n =1099) suggest that, by and large,
                German respondents evaluate the ethical principles as equally important. However, subsequent cluster analysis shows
                that different preference models for ethically designed systems exist among the German population. These clusters sub-
                stantially differ not only in the preferred ethical principles but also in the importance levels of the principles themselves.
                We further describe how these groups are constituted in terms of sociodemographics as well as opinions on artificial
                intelligence. Societal implications, as well as design challenges, are discussed
            \end{abstract}
        \end{itemize}
        
        \item \textbf{Towards Transparency by Design for Artificial Intelligence}
        \begin{itemize}
            \item Title: \textbf{Towards Transparency by Design for Artificial Intelligence}
            \item Authors:
            \begin{itemize}
                \item Heike Felzmann
                \item Eduard Fosch‑Villaronga
                \item Christoph Lutz
                \item Aurelia Tamò‑Larrieux
            \end{itemize}
            \item Year of Publication: 2020
            \item Abstract:
            \begin{abstract}
                In  this  article,  we  develop  the  concept  of  Transparency  by  Design  that  serves  as  
                practical guidance in helping promote the beneficial functions of transparency while  
                mitigating its challenges in automated-decision making (ADM) environments. With 
                the  rise  of  artificial  intelligence  (AI)  and  the  ability  of  AI  systems  to  make  auto-
                mated and self-learned decisions, a call for transparency of how such systems reach 
                decisions  has  echoed  within  academic  and  policy  circles.  The  term  transparency,  
                however,  relates  to  multiple  concepts,  fulfills  many  functions,  and  holds  different  
                promises that struggle to be realized in concrete applications. Indeed, the complex-
                ity  of  transparency  for  ADM  shows  tension  between  transparency  as  a  normative  
                ideal and its translation to practical application. To address this tension, we first con-
                duct  a  review  of  transparency,  analyzing  its  challenges  and  limitations  concerning  
                automated decision-making practices. We then look at the lessons learned from the 
                development  of  Privacy  by  Design,  as  a  basis  for  developing  the  Transparency  by  
                Design principles. Finally, we propose a set of nine principles to cover relevant con-
                textual, technical, informational, and stakeholder-sensitive considerations. Transpar-
                ency  by  Design  is  a  model  that  helps  organizations  design  transparent  AI  systems,  
                by integrating these principles in a step-by-step manner and as an ex-ante value, not 
                as an afterthought.
            \end{abstract}
        \end{itemize}
        
        \item \textbf{Fairness in Design: A Framework for Facilitating Ethical Artificial Intelligence Designs}
        \begin{itemize}
            \item Title: \textbf{Fairness in Design: A Framework for Facilitating Ethical Artificial Intelligence Designs}
            \item Authors:
            \begin{itemize}
                \item Jiehuang Zhang
                \item Ying Shu
                \item Han Yu
            \end{itemize}
            \item Year of Publication: 2020
            \item Abstract:
            \begin{abstract}
                As Artificial Intelligence (AI) and Digital Transformation (DT) technologies become increasingly ubiquitous in modern society, the flaws in their designs are starting to attract attention. AI models have been shown to be susceptible to biases in the training data, especially against underrepresented groups. Although an increasing call for AI solution designers to take fairness into account, the field lacks a design methodology to help AI design teams of members from different backgrounds brainstorm and surface potential fairness issues during the design stage. To address this problem, we propose the Fairness in Design (FID) framework to help AI software designers surface and explore complex fairness-related issues, that otherwise can be overlooked. We explore literature in the field of fairness in AI to narrow down the field into ten major fairness principles, which assist designers in brainstorming around metrics and guide thinking processes about fairness. FID facilitates discussions among design team members, through a game-like approach that is based on a set of prompt cards, to identify and discuss potential concerns from the perspective of various stakeholders. Extensive user studies show that FID is effective at assisting participants in making better decisions about fairness, especially complex issues that involve algorithmic decisions. It has also been found to decrease the barrier of entry for software teams, in terms of the pre-requisite knowledge about fairness, to address fairness issues so that they can make more appropriate related design decisions. The FID methodological framework contributes a novel toolkit to aid in the design and conception process of AI systems, decrease barriers to entry, and assist critical thinking around complex issues surrounding algorithmic systems. The framework is integrated into a step-by-step card game for AI system designers to employ during the design and conception stage of the life-cycle process. FID is a unique decision support framework for software teams interested to create fairness-aware AI solutions.
            \end{abstract}
        \end{itemize}
    \end{itemize}
    

\newpage
\section{Results}
In this section will be summarized the results of the analysis of the papers, with the goal to answer to the question sabout the possible approaches to create trustworty-by-Design mechanisms.

\subsection{What are the approaches used to make the AI Responsible by design?}
Telefonica, a Spanish company, has embraced a comprehensive set of principles to guide their approach to responsible AI. These principles include:

\subsubsection{Fair AI}
Telefonica is committed to ensuring that AI applications lead to fair and non-discriminatory results, considering factors like race, ethnicity, religion, gender, sexual orientation, and disability. They emphasize the importance of assessing the impact of AI algorithms on specific domains.

\subsubsection{Transparent and Explainable AI}
Transparency and explainability are critical to Telefonica. They advocate for clear communication about the data used by AI systems and the purposes they serve. Additionally, they stress the need for providing explanations for AI system decisions.

\subsubsection{Human Centric AI}
Telefonica's AI systems are designed to benefit society, staying under human control and guided by value-based considerations. They ensure that AI applications do not negatively impact human rights or the UN's Sustainable Development Goals.

\subsubsection{Privacy and Security by Design}
Privacy and security are integral to AI system design at Telefonica. These principles align with standard privacy and security practices in organizations, ensuring the protection of personal data.

To implement these principles effectively, Telefonica has developed a robust methodology, which includes:
\begin{itemize}
    \item \textbf{AI Principles}: Establishing core values and boundaries to guide AI development.
    \item \textbf{Questions and Checkpoints}: A set of questions and checkpoints to ensure that all AI principles are considered during the creation process.
    \item \textbf{Tools}: Technical tools to help answer questions and mitigate any problems identified during the development process.
    \item \textbf{Training}: Both technical and non-technical training to equip the team with the necessary knowledge and skills.
    \item \textbf{Governance Model}: Assigning responsibilities and accountabilities within the organization.

\end{itemize}

\subsubsection{Fair AI: Solutions and Challenges}
In the context of Fair AI, various technical tools and solutions have emerged. These include:
\begin{itemize}
    \item \textbf{Fairness Flow Tool}: Tools like Facebook's Fairness Flow tool aim to help detect and mitigate bias in AI systems, particularly by identifying bias in data sets related to sensitive information.
    \item \textbf{IBM's Fairness 360 Toolkit}: IBM's toolkit provides a set of functionalities to address bias, including detecting bias in data sets, identifying correlations between different variables, detecting unbalanced outcomes, and mitigating bias effects.
    \item \textbf{Open Source Tools}: Some of these fairness tools are open-source, such as IBM's offering, tools from Pymetrics, and a tool from the University of Chicago.

\end{itemize}

However, achieving Fair AI comes with its set of challenges, including the causes of bias, various fairness criteria, and the need for evaluation and mitigation techniques.

\subsubsection{Transparent and Explainable AI (xAI)}
Transparent and Explainable AI is essential, especially when dealing with complex "black box" models. Several technical tools and techniques facilitate explainability, including white box models, black box models, model-agnostic methods, and tools for explaining model workings.

\subsubsection{Privacy and Security by Design}
In the context of Privacy and Security by Design, Telefonica addresses privacy risks and security threats. Technical tools are employed to test the risk of re-identification in anonymized datasets and to secure AI systems against potential attacks.

Telefonica's commitment to responsible AI extends to addressing privacy and security concerns, making sure that AI systems are both privacy-respectful and robust against potential attacks.

\newpage
\subsection{What are the approaches adopted to consider fairness since the beginning of the design step?}
FID is presented as a methodological framework for software development teams working on AI solutions. It is designed to help these teams systematically incorporate fairness considerations into their AI systems. This framework ensures that fairness is a fundamental part of the design process rather than an afterthought.

\begin{itemize}
    \item \textbf{Fairness Definitions and Notions}: FID consolidates fairness definitions and notions from the existing literature into ten fundamental fairness principles. These principles are categorized into group fairness and individual fairness. Each of these principles represents a specific aspect of fairness in AI, such as statistical parity, equal opportunity, and more.
    
    \item \textbf{Application Domain Selection}: The FID process starts with the AI design team selecting an application domain. This domain sets the context for the entire session, and it can be a fictional or real-world product or system, depending on the team's needs.
    
    \item \textbf{Categorizing Application Types}: In the context of Human-Computer Interaction literature, the team must categorize the application into one of six types: life-critical systems, industrial and commercial uses, office, home, and entertainment, exploratory and creative, collaborative applications, or socio-technical applications. This categorization helps to tailor fairness considerations to the specific application.
    
    \item \textbf{Identifying Stakeholders}: FID requires the identification of two types of stakeholders: direct and indirect stakeholders. Direct stakeholders are those who use the AI system directly, while indirect stakeholders are affected by its use. This categorization ensures a comprehensive analysis of fairness concerns from various perspectives.
    
    \item \textbf{Fairness Principle Selection}: Each team member randomly selects a fairness principle card from the set of ten. If a selected principle is not relevant to the application domain, the team member can choose to discard it and draw another. This step aligns the fairness metric with the specific context of the AI system.
    
    \item \textbf{Application of Fairness Metrics}: Team members apply the chosen fairness metric to the application domain and analyze its potential implications on stakeholders. They consider how the AI system's decisions might impact these stakeholders, both positively and negatively.
    
    \item \textbf{Anonymous Evaluation and Importance Rating}: Team members compile their responses and randomize them for anonymous evaluation. This process encourages open and honest discussions. After evaluating the responses, the team rates the importance of fairness principles on a Likert scale, further guiding the design process.
    
    \item \textbf{Iterative Process}: The FID process can be repeated or iterated based on the feedback and evolving needs of the AI design team. This adaptability ensures that fairness considerations remain integrated throughout the AI system's development.
    
    \item \textbf{Suitability and Adaptability}: The text suggests that FID is most suitable for projects in the design stage of AI development. It acknowledges that fairness can be dynamic and context-dependent, and FID can be adapted to different application domains and evolving ethical requirements.
\end{itemize}

In summary, Fairness in Design (FID) is a structured and systematic approach to incorporating fairness into AI systems during the design phase. It ensures that ethical considerations are an integral part of AI development, promoting transparency and accountability in the creation of AI technologies.

\newpage
\subsection{What are the approaches to guarantee Transparency-by-Design in AI systems?}
Transparency is a multifaceted concept with varying interpretations across different disciplines. It encompasses notions of explainability, interpretability, openness, accessibility, and visibility, among others. The concept of transparency is of paramount importance in fields such as economics, politics, and law, where it is considered a prerequisite for optimal market functioning, political participation, and administrative legality, respectively. Despite the consensus that transparency is valuable, its precise definition, its intended audience, and its scope of benefits often remain ambiguous.

Our approach to transparency by design is grounded in the belief that understanding transparency purely as an information transfer process falls short of its deeper, value-laden embeddedness within individual agency, relational dynamics, and systemic practices. To this end, we differentiate between three fundamental perspectives on transparency, each highlighting the normative and social aspects of transparency practices: Transparency as a virtue, a relation, and a system.

\subsubsection{Transparency as a Virtue}

From a normative standpoint, transparency is seen as an intrinsically valuable characteristic of agents, systems, or organizations. This involves consistently sharing information about their operations, behaviors, intentions, or considerations. However, this perspective does not specify the target audience for this transparency, leaving room for interpretation.

\subsubsection{Transparency as a Relation}

Viewing transparency through a relational lens emphasizes the importance of considering the audience that will receive and interpret the information. Transparency requirements can vary depending on the likely information recipient, such as individuals whose personal data is processed, various system user categories, regulators, watchdogs, and the general public. Tailoring transparency communications to meet the specific information needs of these different audiences is vital.

\subsubsection{Transparency as a System}

Transparency, when viewed as a system, involves organizational practices, policies, and standards that ensure the implementation of transparency in a structured manner. These systems should allow for the traceability of decisions and should be designed to be responsive to stakeholders' queries and concerns.

Now, let's delve into the stages of transparency by design for Artificial Intelligence (AI) systems, focusing on the need for transparency from the development phase to the deployment phase:

\subsubsection{Phase 1: Information on Data Processing and Analysis}

In this initial phase, the focus is on providing stakeholders with detailed information about what data is processed, how it is processed, and the associated risks. The ultimate goal is to achieve explainability of the AI system and its risks for stakeholders.

\begin{itemize}
    \item \textbf{Explainability}: This involves ensuring that algorithmic decisions and the data driving those decisions can be explained to end-users and other stakeholders in non-technical terms. This includes information about the general functioning of the system, the specific use of data, and individual decisions made by the system.
    
    \item \textbf{Data Processing}: Transparently explaining what data is used by the system and how it is used. This should cover the various stages of data processing and the extent of human discretion and intervention within the system.
    
    \item \textbf{Input and Output Transparency}: Distinguishing between input and output transparency is crucial, considering the potential technical limitations on explaining complex AI decision-making. Transparency requirements may vary depending on whether the information is intended for experts, laypersons, or specific stakeholders.
    
    \item \textbf{Decision-Making Standards}: Detailed explanations about the decision-making criteria and their justifiability, including normative implications. The link between data sources and inferences needs to be critically reflected on.
    
    \item \textbf{Risk Disclosure}: Transparency also extends to explaining the risks associated with AI system operations and the measures taken to mitigate these risks. It should go beyond privacy risks to include issues related to bias, discrimination, and other potential negative consequences.
\end{itemize}

\subsubsection{Phase 2: Organizational and Stakeholder-Oriented Transparency Management}

This final phase is concerned with the organizational and societal response to transparency requirements and expectations. Organizational accountability and adaptability are key in this stage.

\begin{itemize}
    \item \textbf{Inspectability}: The system should enable third-party audits to understand and review its behavior, allowing for retrospective transparency.
    
    \item \textbf{Responsiveness}: The organization must be open to queries and scrutiny from stakeholders, making it easy for them to initiate transparency communications. Responsiveness is not limited to individual cases but should also address the concerns of the general public.
    
    \item \textbf{Reporting}: To promote transparency, regular reports should be published, offering information about system usage, accuracy, and any benchmarking data where relevant. The format and frequency of these reports can vary based on the audience's needs.
\end{itemize}

By following this comprehensive methodology, AI systems can embed transparency by design into their development and deployment processes. This approach ensures that transparency is not merely a one-time endeavor but an ongoing practice that aligns with the varying needs of different stakeholders and maintains a strong focus on ethical and accountable AI. Transparency by design becomes a means to promote trust, mitigate risks, and enhance AI's societal impact.


\newpage
\section{Conclusion}
    % Summarize the main findings and conclusions go here...

\newpage
\section{References}
    % List of cited studies and sources go here...

\end{document}