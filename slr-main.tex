\documentclass{article}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{algorithm2e}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{caption}
\usepackage{float}

\geometry{a4paper, left=25mm, right=25mm, top=30mm, bottom=30mm}

\title{Systematic Scientific Review on Fairness}
\author{Antonio Iannotta}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
    Ethical, fairness, and responsible AI have emerged as pivotal concerns in the design and construction of artificial intelligence systems. This systematic literature review aims to comprehensively analyze the evolving landscape of ethical and responsible AI practices and principles within the domain of AI system development. Over the past decade, the rapid proliferation of AI technologies has underscored the critical need for ethical considerations that extend beyond mere compliance. \\
    The review investigates the frameworks, methodologies, and best practices that advocate for embedding ethical, fairness, and responsible AI principles into the very fabric of AI systems during their design and construction. By examining a wide array of scholarly contributions, this review seeks to elucidate the key ethical challenges faced by AI developers, the strategies employed to integrate fairness and ethical considerations, and the emergent best practices for cultivating responsible AI. \\
    Furthermore, it will explore the impact of ethical and responsible AI design on society, policy, and the broader AI landscape. The findings from this review will provide valuable insights for AI practitioners, researchers, and policymakers, highlighting the importance of ethical and responsible AI as an inherent part of the AI design process.
\end{abstract}
\newpage
\section{Introduction}
Artificial Intelligence (AI) has witnessed remarkable advancements and an unprecedented integration into various aspects of our lives over the past decade. As AI systems become more pervasive, the ethical, fairness, and responsible design of these systems has become an increasingly critical concern. The development of AI is no longer solely about technological innovation and performance optimization but also encompasses a profound sense of social responsibility and ethical awareness. \\
This Systematic Literature Review (SLR) delves into the multifaceted realm of Ethical, Fairness, and Responsible AI by design/construction. The rapid evolution of AI technologies has brought about a shift in the AI paradigm, emphasizing the need for ethical and responsible AI from the outset, during the design and construction phases. It is no longer sufficient to address ethical and fairness considerations as an afterthought or as a post-hoc addition; instead, these principles must be deeply ingrained in the very core of AI systems. \\
The scope of this SLR is to provide a comprehensive analysis of the existing literature, frameworks, methodologies, and best practices surrounding the integration of ethical and responsible AI principles during the design and construction of AI systems. It explores the challenges faced by AI developers, the strategies employed to incorporate fairness and ethics, and the emergent best practices that advocate for responsible AI development. \\
Moreover, the impact of ethical and responsible AI design on society, policy, and the broader AI landscape is a key aspect of this review. As AI systems exert profound influence on various domains, including healthcare, finance, education, and beyond, the ethical and responsible use of AI becomes pivotal for ensuring equitable access and decision-making. \\
Through the synthesis of a diverse body of scholarly contributions, this SLR aspires to elucidate the evolving landscape of Ethical, Fairness, and Responsible AI by design/construction. By synthesizing the knowledge and insights derived from existing research, this review intends to provide valuable guidance to AI practitioners, researchers, and policymakers. The ultimate goal is to underscore the significance of ethical and responsible AI as an integral part of the AI design process, furthering our collective efforts in creating AI systems that prioritize societal well-being and fairness.
\newpage
\section{Methodology}
    \subsection{Search Strategy}
    \textbf{Database Searches:}
\begin{itemize}
    \item ACM Digital Library
    \item Google Scholar
\end{itemize}

\textbf{Search Strings:}
\begin{enumerate}
    \item "AI by design"
    \item "Fair-by-design algorithm"
    \item "Responsible AI by design"
    \item "Fair AI by construction"
    \item "Ethical AI by design"
\end{enumerate}

\textbf{Inclusion and Exclusion Criteria:}
\begin{itemize}
    \item Publication Period: Last 10 years (from 2013 to 2023).
    \item Language: English.
    \item Document Types: Scientific articles, conference papers, conference proceedings, and relevant academic documents.
    \item Excluded: Duplicate documents and articles not relevant to the research topic.
\end{itemize}

This search strategy is designed to retrieve relevant articles published in the last 10 years that address approaches for mitigating bias and ensuring fairness in artificial intelligence systems. Make sure to specify the start and end dates for the publication period. You can use these search strings in the specified databases to identify pertinent sources for your Systematic Literature Review.

    \subsection{Study Selection}
    In this section, we describe the process of study selection, which involved the identification and screening of relevant articles. The goal was to select articles that addressed bias mitigation approaches in the context of specific applications.

    \begin{enumerate}
        \item \textbf{Total Number of Identified Articles:} 10
        \item \textbf{Number of Articles Excluded after Initial Screening:} 7
        
        During the initial screening, 3 articles were excluded due to their strong dependency on the specific application context in which the proposed approaches were intended to be applied.
        
        \item \textbf{Number of Articles Excluded after Secondary Screening:} 1
        
        In the secondary screening, 1 article was excluded due to its deep specificity in a security-by-Design approach.
        
        \item \textbf{Screening Procedures:}
        
        \begin{itemize}
            \item \textbf{Initial Screening:} The initial screening focused on identifying articles that were not closely related to a specific application context. Articles with approaches highly dependent on the application context were excluded.
            
            \item \textbf{Secondary Screening:} During the secondary screening, the goal was to identify articles with a similar methodological approach.
        \end{itemize}
        
        \item \textbf{Exclusion Reasons:}
        
        Articles were excluded if the proposed algorithms and approaches were deemed highly context-specific, tailored to the particular application context presented by the algorithm.
    
    \end{enumerate}
    
    The study selection process ensured that the articles included in the review were relevant to the context of bias mitigation in specific applications, while those heavily dependent on context were excluded.
    \subsection{Data Extraction}
    \begin{itemize}
        \item \textbf{Responsible AI by Design in Practice}
        \begin{itemize}
            \item Title: \textbf{Responsible AI by Design in Practice}
            \item Authors: 
            \begin{itemize}
                \item Richard Benjamins
                \item Alberto Barbado
                \item Daniel Sierra
            \end{itemize}
            \item Year of Publication: 2019
            \item Abstract:
            \begin{abstract}
                Recently, a lot of attention has been given to undesired con-
                sequences of Artificial Intelligence (AI), such as unfair bias 
                leading to discrimination, or the lack of explanations of the 
                results of AI systems. There are several important questions 
                to  answer  before  AI  can  be  deployed  at  scale  in  our  busi-
                nesses  and  societies.  Most  of  these  issues  are  being  dis-
                cussed  by  experts  and the  wider communities,  and  it  seems 
                there is broad consensus on where they come from. There is, 
                however,  less  consensus  on,  and  experience  with  how  to 
                practically  deal  with  those  issues  in  organizations  that  de-
                velop  and  use  AI,  both  from  a  technical  and  organizational 
                perspective. In this paper, we discuss the practical case of a 
                large  organization  that  is  putting  in  place  a  company-wide 
                methodology to minimize the risk of undesired consequenc-
                es  of  AI.  We  hope  that  other  organizations  can  learn  from 
                this  and  that  our  experience  contributes  to  making  the  best 
                of AI while minimizing its risks.
            \end{abstract}
        \end{itemize}
        
        \item \textbf{A Multidisciplinary Survey and Framework for Design and
        Evaluation of Explainable AI Systems}
        \begin{itemize}
            \item Title: \textbf{A Multidisciplinary Survey and Framework for Design and
            Evaluation of Explainable AI Systems}
            \item Authors:
            \begin{itemize}
                \item SINA MOHSENI
                \item NILOOFAR ZAREI
                \item ERIC D. RAGAN
            \end{itemize}
            \item Year of Publication: 2021
            \item Abstract:
            \begin{abstract}
                The need for interpretable and accountable intelligent systems grows along with the prevalence of artificial
                intelligence (AI) applications used in everyday life. Explainable AI (XAI) systems are intended to self-
                explain the reasoning behind system decisions and predictions. Researchers from different disciplines work
                together to define, design, and evaluate explainable systems. However, scholars from different disciplines
                focus on different objectives and fairly independent topics of XAI research, which poses challenges for iden-
                tifying appropriate design and evaluation methodology and consolidating knowledge across efforts. To this
                end, this article presents a survey and framework intended to share knowledge and experiences of XAI design
                and evaluation methods across multiple disciplines. Aiming to support diverse design goals and evaluation
                methods in XAI research, after a thorough review of XAI related papers in the fields of machine learning, vi-
                sualization, and human-computer interaction, we present a categorization of XAI design goals and evaluation
                methods. Our categorization presents the mapping between design goals for different XAI user groups and
                their evaluation methods. From our findings, we develop a framework with step-by-step design guidelines
                paired with evaluation methods to close the iterative design and evaluation cycles in multidisciplinary XAI
                teams. Further, we provide summarized ready-to-use tables of evaluation methods and recommendations for
                different goals in XAI research
            \end{abstract}
        \end{itemize}
        
        \item \textbf{Bridging the Gap Between AI and 
        Explainability in the GDPR: Towards 
        Trustworthiness-by-Design in 
        Automated Decision-Making}
        \begin{itemize}
            \item Title: \textbf{Bridging the Gap Between AI and 
            Explainability in the GDPR: Towards 
            Trustworthiness-by-Design in 
            Automated Decision-Making}
            \item Authors:
            \begin{itemize}
                \item Ronan Hamon
                \item Henrik Junklewitz
                \item Ignacio Sanchez
                \item Gianclaudio Malgieri
                \item Paul De Hert
            \end{itemize}
            \item Year of Publication: 2022
            \item Abstract:
            \begin{abstract}
                Can satisfactory explanations for complex machine learning models be achieved in high-risk automated decision-making? How can such explanations be integrated into a data protection framework safeguarding a right to explanation? This article explores from an interdisciplinary point of view the connection between existing legal requirements for the explainability of AI systems set out in the General Data Protection Regulation (GDPR) and the current state of the art in the field of explainable AI. It studies the challenges of providing human-legible explanations for current and future AI-based decision-making systems in practice, based on two scenarios of automated decision-making in credit scoring risks and medical diagnosis of COVID-19. These scenarios exemplify the trend towards increasingly complex machine learning algorithms in automated decision-making, both in terms of data and models. Current machine learning techniques, in particular those based on deep learning, are unable to make clear causal links between input data and final decisions. This represents a limitation for providing exact, human-legible reasons behind specific decisions and presents a serious challenge to the provision of satisfactory, fair, and transparent explanations. Therefore, the conclusion is that the quality of explanations might not be considered as an adequate safeguard for automated decision-making processes under the GDPR. Accordingly, additional tools should be considered to complement explanations. These could include algorithmic impact assessments, other forms of algorithmic justifications based on broader AI principles, and new technical developments in trustworthy AI. This suggests that eventually all of these approaches would need to be considered as a whole.
            \end{abstract}
        \end{itemize}
        
        \item \textbf{“EQUALITY AND PRIVACY BY DESIGN”:         
        A NEW MODEL OF ARTIFICIAL 
        INTELLIGENCE DATA TRANSPARENCY VIA 
        AUDITING, CERTIFICATION, AND SAFE 
        HARBOR REGIMES}
        \begin{itemize}
            \item Title: \textbf{“EQUALITY AND PRIVACY BY DESIGN”:         
            A NEW MODEL OF ARTIFICIAL 
            INTELLIGENCE DATA TRANSPARENCY VIA 
            AUDITING, CERTIFICATION, AND SAFE 
            HARBOR REGIMES}
            \item Authors:
            \begin{itemize}
                \item Shlomit Yanisky-Ravid
                \item Sean K. Hallisey
            \end{itemize}
            \item Year of Publication: 2019
            \item Abstract:
            \begin{abstract}
                Artificial Intelligence and Machine Learning (AI) are often 
                described as technological breakthroughs that will completely 
                transform our society and economy.  AI systems have been 
                implemented everywhere, from medicine, transportation, finance, art, 
                to  legal  and  social  spheres,  and  even  in  weapons  development.    In  
                many sectors, AI systems have already started making decisions 
                previously  made  by  humans.    Promising  as  AI  systems  may  be,  they  
                also pose urgent challenges to our everyday life.  While much 
                attention  has  concerned  AI’s  legal  implications,  the  literature  suffers  
                from  a  lack  of  solutions  that  account  for  both  legal  and  engineering  
                practices and constraints.  This leaves technology firms without guidelines and increases the risk of societal harm.  It also means that 
                policymakers and judges operate without a regulatory regime to turn 
                to  when  addressing  these  novel  and  unpredictable  outcomes.    This  
                Article  tries  to  fill  the  void  by  focusing  on  data  rather  than  on  the  
                software and programmers.  It suggests a new model that stems from 
                a recognition of the significant role that the data plays in the    
                development and functioning of AI systems. Data is the most important aspect of teaching AI systems to 
                operate. AI  algorithms  begin  with  a  massive  preexisting  dataset, 
                which  data  providers  use  to  train  the  system.    But  the  data  that  AI  
                systems  “swallow”  can  be  illegal,  discriminatory,  altered,  unreliable,  
                or simply incomplete.  Thus, the more data fed to the AI systems, the 
                higher  the  likelihood  that  they  could  produce  biased,  discriminatory  
                decisions and violate privacy rights.  The Article discusses how 
                discrimination  can  arise,  even  inadvertently,  from  the  operation  of  
                “trusted” and “objective” AI systems. To  address  this  problem,  this  Article  proposes  a  new  AI  Data 
                Transparency Model that focuses on disclosure of data rather than, as 
                some  scholars  argue,  focusing  on  the  initial  software  program  and  
                programmers.  The Model includes an auditing regime and a 
                certification  program,  run  either  by  a  governmental  body  or,  in  the  
                absence of such entity, by private institutions.  This Model will 
                encourage the industry to take proactive steps to ensure and publicize 
                that  datasets  are  trustworthy.    The  suggested  Model  includes  a  safe  
                harbor, which incentivizes firms to implement transparency 
                recommendations  even  without  massive  regulatory  oversight.    From  
                an  engineering  point  of  view,  the  Model  recognizes  data  providers  
                and  big  data  as  the  most  important  components  in  the  process  of  
                creating, training and operating AI systems.  Even more importantly, 
                the Model is technologically feasible because data can be easily 
                absorbed and kept by a technological tool.  Further, this Model is also 
                practically feasible because it follows already existing legal 
                frameworks of data transparency, such as the ones being 
                implemented by the FDA and the SEC. Improving transparency in data systems would result in less 
                harmful  AI  systems,  better  protect  societal  rights  and  norms,  and  
                produc e  improved  outcomes  in  this  emerging  field,  especially  for  
                minority  communities  that  often  lack  resources  or  representation  to  
                challenge AI systems.  Increased transparency of the data used while 
                developing,  training  or  operating  AI  systems  would  mitigate  and  
                reduce these harms.  Additionally, to better identify the risks of faulty 
                data,  industry  players  must  conduct  critical  evaluations  and  audits  of  
                the  data  used  to  train  AI  systems;  one  way  to  incentivize  this  is  a  certification system to publicize good-faith efforts to reduce the 
                possibility  of  discriminatory  outcomes  and  privacy  violations  in  AI  
                systems.  This Article strives to incentivize the creation of new 
                standards, which the industry could implement from the genesis of AI 
                systems  to  mitigate  the  possibility  of  harm,  rather  than  post-hoc 
                assignments of liability.
            \end{abstract}
        \end{itemize}
        
        \item \textbf{Artificial intelligence ethics by design.
        Evaluating public perception on the
        importance of ethical design principles
        of artificial intelligence}
        \begin{itemize}
            \item Title: \textbf{Artificial intelligence ethics by design.
            Evaluating public perception on the
            importance of ethical design principles
            of artificial intelligence}
            \item Authors:
            \begin{itemize}
                \item Kimon Kieslich
                \item Birte Keller
                \item Christopher Starke
            \end{itemize}
            \item Year of Publication: 2022
            \item Abstract:
            \begin{abstract}
                Despite the immense societal importance of ethically designing artificial intelligence, little research on the public percep-
                tions of ethical artificial intelligence principles exists. This becomes even more striking when considering that ethical arti-
                ficial intelligence development has the aim to be human-centric and of benefit for the whole society. In this study, we
                investigate how ethical principles (explainability, fairness, security, accountability, accuracy, privacy, and machine auton-
                omy) are weighted in comparison to each other. This is especially important, since simultaneously considering ethical
                principles is not only costly, but sometimes even impossible, as developers must make specific trade-off decisions. In
                this paper, we give first answers on the relative importance of ethical principles given a specific use case—the use of arti-
                ficial intelligence in tax fraud detection. The results of a large conjoint survey (n =1099) suggest that, by and large,
                German respondents evaluate the ethical principles as equally important. However, subsequent cluster analysis shows
                that different preference models for ethically designed systems exist among the German population. These clusters sub-
                stantially differ not only in the preferred ethical principles but also in the importance levels of the principles themselves.
                We further describe how these groups are constituted in terms of sociodemographics as well as opinions on artificial
                intelligence. Societal implications, as well as design challenges, are discussed
            \end{abstract}
        \end{itemize}
        
        \item \textbf{Towards Transparency by Design for Artificial Intelligence}
        \begin{itemize}
            \item Title: \textbf{Towards Transparency by Design for Artificial Intelligence}
            \item Authors:
            \begin{itemize}
                \item Heike Felzmann
                \item Eduard Fosch‑Villaronga
                \item Christoph Lutz
                \item Aurelia Tamò‑Larrieux
            \end{itemize}
            \item Year of Publication: 2020
            \item Abstract
            \begin{abstract}
                In  this  article,  we  develop  the  concept  of  Transparency  by  Design  that  serves  as  
                practical guidance in helping promote the beneficial functions of transparency while  
                mitigating its challenges in automated-decision making (ADM) environments. With 
                the  rise  of  artificial  intelligence  (AI)  and  the  ability  of  AI  systems  to  make  auto-
                mated and self-learned decisions, a call for transparency of how such systems reach 
                decisions  has  echoed  within  academic  and  policy  circles.  The  term  transparency,  
                however,  relates  to  multiple  concepts,  fulfills  many  functions,  and  holds  different  
                promises that struggle to be realized in concrete applications. Indeed, the complex-
                ity  of  transparency  for  ADM  shows  tension  between  transparency  as  a  normative  
                ideal and its translation to practical application. To address this tension, we first con-
                duct  a  review  of  transparency,  analyzing  its  challenges  and  limitations  concerning  
                automated decision-making practices. We then look at the lessons learned from the 
                development  of  Privacy  by  Design,  as  a  basis  for  developing  the  Transparency  by  
                Design principles. Finally, we propose a set of nine principles to cover relevant con-
                textual, technical, informational, and stakeholder-sensitive considerations. Transpar-
                ency  by  Design  is  a  model  that  helps  organizations  design  transparent  AI  systems,  
                by integrating these principles in a step-by-step manner and as an ex-ante value, not 
                as an afterthought.
            \end{abstract}
        \end{itemize}
    \end{itemize}

    \subsection{Quality Assessment}
        % Quality assessment criteria and process details go here...

    \subsection{Data Synthesis}
        % Data synthesis methods and approach details go here...

    \subsection{Reporting Guidelines}
        % Mention if any specific reporting guidelines were followed...

\newpage
\section{Results}
    % Results summary, tables, charts, and key findings go here...

\newpage
\section{Discussion}
    % Interpretation of results, implications, limitations, and future research areas go here...

\newpage
\section{Conclusion}
    % Summarize the main findings and conclusions go here...

\newpage
\section{References}
    % List of cited studies and sources go here...

\section{Appendices}
    % Include supplementary materials if necessary...

\section{Acknowledgments}
    % Acknowledgments go here...

\end{document}