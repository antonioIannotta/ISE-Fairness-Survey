\documentclass{article}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{algorithm2e}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{caption}
\usepackage{float}

\geometry{a4paper, left=25mm, right=25mm, top=30mm, bottom=30mm}

\title{Systematic Scientific Review on Fairness}
\author{Antonio Iannotta}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
    The widespread integration of intelligent systems based on artificial intelligence (AI) and machine learning (ML) in sectors such as healthcare, finance, and justice has raised concerns about fairness and discrimination. This Systematic Literature Review (SLR) aims to examine existing research on "fairness" in intelligent systems, with a focus on challenges, proposed solutions, and tools used to address bias in AI/ML-based systems.
    The SLR reviews a wide range of scientific publications, including articles, empirical studies, and guidelines, to provide a comprehensive overview of the current state of research. Commonly used approaches and metrics for measuring and mitigating bias in data and algorithms are examined. Additionally, ethical and social challenges associated with the implementation of fair intelligent systems are analyzed, along with key recommendations.
    This work contributes to the understanding of fairness challenges in intelligent systems and serves as a reference for future research and development in this crucial field to ensure fairer and more inclusive systems. \\
    \textbf{Keywords:} fairness, intelligent systems, artificial intelligence, machine learning, discrimination, bias.
\end{abstract}
\newpage
\section{Introduction}
The widespread integration of artificial intelligence (AI) systems across critical domains, including healthcare, finance, and justice, has introduced new opportunities while raising critical questions about fairness and discrimination. In the real world, AI systems may inherit biases and discrimination from the training data, jeopardizing fairness and objectivity in decision-making. The quest for solutions to mitigate bias in AI systems and ensure fairness has become a paramount concern. \\
This Systematic Literature Review (SLR) aims to comprehensively examine the evolution of approaches designed to ensure fairness in AI systems over the past decade. The issue of fairness has emerged as a central theme in the discourse on AI responsibility and ethics, as it directly influences the societal impact of these systems. \\
Fairness is a fundamental concept in ensuring that AI systems are just and equitable. Discrimination and bias can lead to unjust decisions with significant consequences for those affected. Addressing these issues is imperative to ensure that technological advancements in AI are inclusive and uphold fundamental rights. \\
In this SLR, we seek to address the following research question: How have approaches to ensuring fairness in AI systems evolved over the years? We will examine research progress, newly developed methodologies, and outstanding challenges in this rapidly evolving field. \\
This study focuses on the past decade, aiming to capture the most recent developments and emerging trends concerning fairness in AI systems. \\
This SLR assumes critical importance as it provides an up-to-date and in-depth analysis of the methodologies and approaches used to ensure fairness in AI systems. Such an analysis is pivotal for understanding how AI can be developed responsibly and in alignment with ethical principles.

\newpage
\section{Methodology}
    \subsection{Search Strategy}
    \textbf{Database Searches:}
\begin{itemize}
    \item ACM Digital Library
    \item Google Scholar
\end{itemize}

\textbf{Search Strings:}
\begin{enumerate}
    \item "Approaches for mitigating bias in artificial intelligence systems"
    \item "Methods for ensuring fairness in AI systems"
    \item "Techniques to reduce bias in machine learning"
    \item "AI system bias mitigation methodologies"
    \item "Fairness and bias in artificial intelligence"
    \item "Ethical AI and bias reduction"
    \item "AI system fairness over the last 10 years"
    \item "Bias mitigation strategies in AI"
\end{enumerate}

\textbf{Inclusion and Exclusion Criteria:}
\begin{itemize}
    \item Publication Period: Last 10 years (from 2013 to 2023).
    \item Language: English.
    \item Document Types: Scientific articles, conference papers, conference proceedings, and relevant academic documents.
    \item Excluded: Duplicate documents and articles not relevant to the research topic.
\end{itemize}

This search strategy is designed to retrieve relevant articles published in the last 10 years that address approaches for mitigating bias and ensuring fairness in artificial intelligence systems. Make sure to specify the start and end dates for the publication period. You can use these search strings in the specified databases to identify pertinent sources for your Systematic Literature Review.

    \subsection{Study Selection}
    In this section, we describe the process of study selection, which involved the identification and screening of relevant articles. The goal was to select articles that addressed bias mitigation approaches in the context of specific applications.

    \begin{enumerate}
        \item \textbf{Total Number of Identified Articles:} 25
        \item \textbf{Number of Articles Excluded after Initial Screening:} 10
        
        During the initial screening, 10 articles were excluded due to their strong dependency on the specific application context in which the proposed approaches were intended to be applied.
        
        \item \textbf{Number of Articles Excluded after Secondary Screening:} 0
        
        In the secondary screening, no articles were excluded. The screening process aimed to identify articles sharing the same methodological approach.
        
        \item \textbf{Screening Procedures:}
        
        \begin{itemize}
            \item \textbf{Initial Screening:} The initial screening focused on identifying articles that were not closely related to a specific application context. Articles with approaches highly dependent on the application context were excluded.
            
            \item \textbf{Secondary Screening:} During the secondary screening, the goal was to identify articles with a similar methodological approach.
        \end{itemize}
        
        \item \textbf{Exclusion Reasons:}
        
        Articles were excluded if the proposed algorithms and approaches were deemed highly context-specific, tailored to the particular application context presented by the algorithm.
    
    \end{enumerate}
    
    The study selection process ensured that the articles included in the review were relevant to the context of bias mitigation in specific applications, while those heavily dependent on context were excluded.
    \subsection{Data Extraction}
    \begin{itemize}
        \item \textbf{THE VARIATIONAL FAIR AUTOENCODER}
        \begin{itemize}
            \item Title: \textbf{The variational fair autoencoder}
            \item Authors: 
            \begin{itemize}
                \item Christos Louizos
                \item Kevin Swersky
                \item Yujia Li
                \item Max Welling
                \item Richard Zemel
            \end{itemize}
            \item Year of Publication: 2017
            \item Abstract:
            \begin{abstract}
                We investigate the problem of learning representations that are invariant to cer-
                tain nuisance or sensitive factors of variation in the data while retaining as much
                of the remaining information as possible. Our model is based on a variational
                autoencoding architecture (Kingma \& Welling, 2014; Rezende et al., 2014) with
                priors that encourage independence between sensitive and latent factors of varia-
                tion. Any subsequent processing, such as classification, can then be performed on
                this purged latent representation. To remove any remaining dependencies we in-
                corporate an additional penalty term based on the “Maximum Mean Discrepancy”
                (MMD) (Gretton et al., 2006) measure. We discuss how these architectures can
                be efficiently trained on data and show in experiments that this method is more
                effective than previous work in removing unwanted sources of variation while
                maintaining informative latent representations.
            \end{abstract}
        \end{itemize}
        
        \item \textbf{Algorithmic decision making and the cost of fairness}
        \begin{itemize}
            \item Title: \textbf{Algorithmic decision making and the cost of fairness}
            \item Authors:
            \begin{itemize}
                \item Sam Corbett-Davies
                \item Emma Pierson
                \item Avi Feller
                \item Sharad Goel
                \item Aziz Huq
            \end{itemize}
            \item Year of Publication: 2017
            \item Abstract:
            \begin{abstract}
                Algorithms are now regularly used to decide whether defendants
                awaiting trial are too dangerous to be released back into the com-
                munity. In some cases, black defendants are substantially more
                likely than white defendants to be incorrectly classified as high
                risk. To mitigate such disparities, several techniques have recently
                been proposed to achieve algorithmic fairness. Here we reformulate
                algorithmic fairness as constrained optimization: the objective is to
                maximize public safety while satisfying formal fairness constraints
                designed to reduce racial disparities. We show that for several past
                definitions of fairness, the optimal algorithms that result require de- 
                taining defendants above race-specific risk thresholds. We further
                show that the optimal unconstrained algorithm requires applying
                a single, uniform threshold to all defendants. The unconstrained
                algorithm thus maximizes public safety while also satisfying one
                important understanding of equality: that all individuals are held
                to the same standard, irrespective of race. Because the optimal
                constrained and unconstrained algorithms generally differ, there is
                tension between improving public safety and satisfying prevailing
                notions of algorithmic fairness. By examining data from Broward
                County, Florida, we show that this trade-off can be large in prac-
                tice. We focus on algorithms for pretrial release decisions, but the
                principles we discuss apply to other domains, and also to human
                decision makers carrying out structured decision rules.
            \end{abstract}
        \end{itemize}
        
        \item \textbf{Avoiding Discrimination through Causal Reasoning}
        \begin{itemize}
            \item Title
            \item Authors
            \item Year of Publication
            \item Abstract
        \end{itemize}
        
        \item \textbf{Data Decisions and Theoretical Implications when Adversarially Learning Fair Representations}
        \begin{itemize}
            \item Title
            \item Authors
            \item Year of Publication
            \item Abstract
        \end{itemize}
        
        \item \textbf{Proxy Discrimination in Data-Driven Systems}
        \begin{itemize}
            \item Title
            \item Authors
            \item Year of Publication
            \item Abstract
        \end{itemize}
        
        \item \textbf{A Reductions Approach to Fair Classification}
        \begin{itemize}
            \item Title
            \item Authors
            \item Year of Publication
            \item Abstract
        \end{itemize}
        
        \item \textbf{Proxy Fairness}
        \begin{itemize}
            \item Title
            \item Authors
            \item Year of Publication
            \item Abstract
        \end{itemize}
        
        \item \textbf{BIAS MITIGATION POST-PROCESSING FOR INDIVIDUAL AND GROUP FAIRNESS}
        \begin{itemize}
            \item Title
            \item Authors
            \item Year of Publication
            \item Abstract
        \end{itemize}
        
        \item \textbf{One-network Adversarial Fairness}
        \begin{itemize}
            \item Title
            \item Authors
            \item Year of Publication
            \item Abstract
        \end{itemize}
        
        \item \textbf{k-NN as an Implementation of Situation Testing for Discrimination Discovery and Prevention}
        \begin{itemize}
            \item Title
            \item Authors
            \item Year of Publication
            \item Abstract
        \end{itemize}
        
        \item \textbf{Fairness definitions explained}
        \begin{itemize}
            \item Title
            \item Authors
            \item Year of Publication
            \item Abstract
        \end{itemize}
        
        \item \textbf{Equality of Opportunity in Supervised Learning}
        \begin{itemize}
            \item Title
            \item Authors
            \item Year of Publication
            \item Abstract
        \end{itemize}
        
        \item \textbf{Recycling Privileged Learning and Distribution Matching for Fairness}
        \begin{itemize}
            \item Title
            \item Authors
            \item Year of Publication
            \item Abstract
        \end{itemize}
        
        \item \textbf{Path-Specific Counterfactual Fairness}
        \begin{itemize}
            \item Title
            \item Authors
            \item Year of Publication
            \item Abstract
        \end{itemize}
        
        \item \textbf{Fairness Constraints: Mechanisms for Fair Classification}
        \begin{itemize}
            \item Title
            \item Authors
            \item Year of Publication
            \item Abstract
        \end{itemize}
    \end{itemize}

    \subsection{Quality Assessment}
        % Quality assessment criteria and process details go here...

    \subsection{Data Synthesis}
        % Data synthesis methods and approach details go here...

    \subsection{Reporting Guidelines}
        % Mention if any specific reporting guidelines were followed...

\newpage
\section{Results}
    % Results summary, tables, charts, and key findings go here...

\newpage
\section{Discussion}
    % Interpretation of results, implications, limitations, and future research areas go here...

\newpage
\section{Conclusion}
    % Summarize the main findings and conclusions go here...

\newpage
\section{References}
    % List of cited studies and sources go here...

\section{Appendices}
    % Include supplementary materials if necessary...

\section{Acknowledgments}
    % Acknowledgments go here...

\end{document}