\documentclass{article}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{algorithm2e}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{caption}
\usepackage{float}

\geometry{a4paper, left=25mm, right=25mm, top=30mm, bottom=30mm}

\title{Systematic Scientific Review on Fairness}
\author{Antonio Iannotta}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
    Ethical, fairness, and responsible AI have emerged as pivotal concerns in the design and construction of artificial intelligence systems. This systematic literature review aims to comprehensively analyze the evolving landscape of ethical and responsible AI practices and principles within the domain of AI system development. Over the past decade, the rapid proliferation of AI technologies has underscored the critical need for ethical considerations that extend beyond mere compliance. \\
    The review investigates the frameworks, methodologies, and best practices that advocate for embedding ethical, fairness, and responsible AI principles into the very fabric of AI systems during their design and construction. By examining a wide array of scholarly contributions, this review seeks to elucidate the key ethical challenges faced by AI developers, the strategies employed to integrate fairness and ethical considerations, and the emergent best practices for cultivating responsible AI. \\
    Furthermore, it will explore the impact of ethical and responsible AI design on society, policy, and the broader AI landscape. The findings from this review will provide valuable insights for AI practitioners, researchers, and policymakers, highlighting the importance of ethical and responsible AI as an inherent part of the AI design process.
\end{abstract}
\newpage
\section{Introduction}
Artificial Intelligence (AI) has witnessed remarkable advancements and an unprecedented integration into various aspects of our lives over the past decade. As AI systems become more pervasive, the ethical, fairness, and responsible design of these systems has become an increasingly critical concern. The development of AI is no longer solely about technological innovation and performance optimization but also encompasses a profound sense of social responsibility and ethical awareness. \\
This Systematic Literature Review (SLR) delves into the multifaceted realm of Ethical, Fairness, and Responsible AI by design/construction. The rapid evolution of AI technologies has brought about a shift in the AI paradigm, emphasizing the need for ethical and responsible AI from the outset, during the design and construction phases. It is no longer sufficient to address ethical and fairness considerations as an afterthought or as a post-hoc addition; instead, these principles must be deeply ingrained in the very core of AI systems. \\
The scope of this SLR is to provide a comprehensive analysis of the existing literature, frameworks, methodologies, and best practices surrounding the integration of ethical and responsible AI principles during the design and construction of AI systems. It explores the challenges faced by AI developers, the strategies employed to incorporate fairness and ethics, and the emergent best practices that advocate for responsible AI development. \\
Moreover, the impact of ethical and responsible AI design on society, policy, and the broader AI landscape is a key aspect of this review. As AI systems exert profound influence on various domains, including healthcare, finance, education, and beyond, the ethical and responsible use of AI becomes pivotal for ensuring equitable access and decision-making. \\
Through the synthesis of a diverse body of scholarly contributions, this SLR aspires to elucidate the evolving landscape of Ethical, Fairness, and Responsible AI by design/construction. By synthesizing the knowledge and insights derived from existing research, this review intends to provide valuable guidance to AI practitioners, researchers, and policymakers. The ultimate goal is to underscore the significance of ethical and responsible AI as an integral part of the AI design process, furthering our collective efforts in creating AI systems that prioritize societal well-being and fairness.
\newpage
\section{Methodology}
    \subsection{Search Strategy}
    \textbf{Database Searches:}
\begin{itemize}
    \item ACM Digital Library
    \item Google Scholar
\end{itemize}

\textbf{Search Strings:}
\begin{enumerate}
    \item "AI by design"
    \item "Fair-by-design algorithm"
    \item "Responsible AI by design"
    \item "Fair AI by construction"
    \item "Ethical AI by design"
\end{enumerate}

\textbf{Inclusion and Exclusion Criteria:}
\begin{itemize}
    \item Publication Period: Last 10 years (from 2013 to 2023).
    \item Language: English.
    \item Document Types: Scientific articles, conference papers, conference proceedings, and relevant academic documents.
    \item Excluded: Duplicate documents and articles not relevant to the research topic.
\end{itemize}

This search strategy is designed to retrieve relevant articles published in the last 10 years that address approaches for mitigating bias and ensuring fairness in artificial intelligence systems. Make sure to specify the start and end dates for the publication period. You can use these search strings in the specified databases to identify pertinent sources for your Systematic Literature Review.

    \subsection{Study Selection}
    In this section, we describe the process of study selection, which involved the identification and screening of relevant articles. The goal was to select articles that addressed bias mitigation approaches in the context of specific applications.

    \begin{enumerate}
        \item \textbf{Total Number of Identified Articles:} 10
        \item \textbf{Number of Articles Excluded after Initial Screening:} 5
        
        During the initial screening, 3 articles were excluded due to their strong dependency on the specific application context in which the proposed approaches were intended to be applied.
        
        \item \textbf{Number of Articles Excluded after Secondary Screening:} 1
        
        In the secondary screening, 1 article was excluded due to its deep specificity in a security-by-Design approach.
        
        \item \textbf{Screening Procedures:}
        
        \begin{itemize}
            \item \textbf{Initial Screening:} The initial screening focused on identifying articles that were not closely related to a specific application context. Articles with approaches highly dependent on the application context were excluded.
            
            \item \textbf{Secondary Screening:} During the secondary screening, the goal was to identify articles with a similar methodological approach.
        \end{itemize}
        
        \item \textbf{Exclusion Reasons:}
        
        Articles were excluded if the proposed algorithms and approaches were deemed highly context-specific, tailored to the particular application context presented by the algorithm.
    
    \end{enumerate}
    
    The study selection process ensured that the articles included in the review were relevant to the context of bias mitigation in specific applications, while those heavily dependent on context were excluded.
    \subsection{Data Extraction}
    \begin{itemize}
        \item \textbf{Responsible AI by Design in Practice}
        \begin{itemize}
            \item Title: \textbf{Responsible AI by Design in Practice}
            \item Authors: 
            \begin{itemize}
                \item Richard Benjamins
                \item Alberto Barbado
                \item Daniel Sierra
            \end{itemize}
            \item Year of Publication: 2019
            \item Abstract:
            \begin{abstract}
                Recently, a lot of attention has been given to undesired con-
                sequences of Artificial Intelligence (AI), such as unfair bias 
                leading to discrimination, or the lack of explanations of the 
                results of AI systems. There are several important questions 
                to  answer  before  AI  can  be  deployed  at  scale  in  our  busi-
                nesses  and  societies.  Most  of  these  issues  are  being  dis-
                cussed  by  experts  and the  wider communities,  and  it  seems 
                there is broad consensus on where they come from. There is, 
                however,  less  consensus  on,  and  experience  with  how  to 
                practically  deal  with  those  issues  in  organizations  that  de-
                velop  and  use  AI,  both  from  a  technical  and  organizational 
                perspective. In this paper, we discuss the practical case of a 
                large  organization  that  is  putting  in  place  a  company-wide 
                methodology to minimize the risk of undesired consequenc-
                es  of  AI.  We  hope  that  other  organizations  can  learn  from 
                this  and  that  our  experience  contributes  to  making  the  best 
                of AI while minimizing its risks.
            \end{abstract}
        \end{itemize}
        
        
        \item \textbf{Bridging the Gap Between AI and 
        Explainability in the GDPR: Towards 
        Trustworthiness-by-Design in 
        Automated Decision-Making}
        \begin{itemize}
            \item Title: \textbf{Bridging the Gap Between AI and 
            Explainability in the GDPR: Towards 
            Trustworthiness-by-Design in 
            Automated Decision-Making}
            \item Authors:
            \begin{itemize}
                \item Ronan Hamon
                \item Henrik Junklewitz
                \item Ignacio Sanchez
                \item Gianclaudio Malgieri
                \item Paul De Hert
            \end{itemize}
            \item Year of Publication: 2022
            \item Abstract:
            \begin{abstract}
                Can satisfactory explanations for complex machine learning models be achieved in high-risk automated decision-making? How can such explanations be integrated into a data protection framework safeguarding a right to explanation? This article explores from an interdisciplinary point of view the connection between existing legal requirements for the explainability of AI systems set out in the General Data Protection Regulation (GDPR) and the current state of the art in the field of explainable AI. It studies the challenges of providing human-legible explanations for current and future AI-based decision-making systems in practice, based on two scenarios of automated decision-making in credit scoring risks and medical diagnosis of COVID-19. These scenarios exemplify the trend towards increasingly complex machine learning algorithms in automated decision-making, both in terms of data and models. Current machine learning techniques, in particular those based on deep learning, are unable to make clear causal links between input data and final decisions. This represents a limitation for providing exact, human-legible reasons behind specific decisions and presents a serious challenge to the provision of satisfactory, fair, and transparent explanations. Therefore, the conclusion is that the quality of explanations might not be considered as an adequate safeguard for automated decision-making processes under the GDPR. Accordingly, additional tools should be considered to complement explanations. These could include algorithmic impact assessments, other forms of algorithmic justifications based on broader AI principles, and new technical developments in trustworthy AI. This suggests that eventually all of these approaches would need to be considered as a whole.
            \end{abstract}
        \end{itemize}
        
        \item \textbf{Artificial intelligence ethics by design.
        Evaluating public perception on the
        importance of ethical design principles
        of artificial intelligence}
        \begin{itemize}
            \item Title: \textbf{Artificial intelligence ethics by design.
            Evaluating public perception on the
            importance of ethical design principles
            of artificial intelligence}
            \item Authors:
            \begin{itemize}
                \item Kimon Kieslich
                \item Birte Keller
                \item Christopher Starke
            \end{itemize}
            \item Year of Publication: 2022
            \item Abstract:
            \begin{abstract}
                Despite the immense societal importance of ethically designing artificial intelligence, little research on the public percep-
                tions of ethical artificial intelligence principles exists. This becomes even more striking when considering that ethical arti-
                ficial intelligence development has the aim to be human-centric and of benefit for the whole society. In this study, we
                investigate how ethical principles (explainability, fairness, security, accountability, accuracy, privacy, and machine auton-
                omy) are weighted in comparison to each other. This is especially important, since simultaneously considering ethical
                principles is not only costly, but sometimes even impossible, as developers must make specific trade-off decisions. In
                this paper, we give first answers on the relative importance of ethical principles given a specific use case—the use of arti-
                ficial intelligence in tax fraud detection. The results of a large conjoint survey (n =1099) suggest that, by and large,
                German respondents evaluate the ethical principles as equally important. However, subsequent cluster analysis shows
                that different preference models for ethically designed systems exist among the German population. These clusters sub-
                stantially differ not only in the preferred ethical principles but also in the importance levels of the principles themselves.
                We further describe how these groups are constituted in terms of sociodemographics as well as opinions on artificial
                intelligence. Societal implications, as well as design challenges, are discussed
            \end{abstract}
        \end{itemize}
        
        \item \textbf{Towards Transparency by Design for Artificial Intelligence}
        \begin{itemize}
            \item Title: \textbf{Towards Transparency by Design for Artificial Intelligence}
            \item Authors:
            \begin{itemize}
                \item Heike Felzmann
                \item Eduard Fosch‑Villaronga
                \item Christoph Lutz
                \item Aurelia Tamò‑Larrieux
            \end{itemize}
            \item Year of Publication: 2020
            \item Abstract
            \begin{abstract}
                In  this  article,  we  develop  the  concept  of  Transparency  by  Design  that  serves  as  
                practical guidance in helping promote the beneficial functions of transparency while  
                mitigating its challenges in automated-decision making (ADM) environments. With 
                the  rise  of  artificial  intelligence  (AI)  and  the  ability  of  AI  systems  to  make  auto-
                mated and self-learned decisions, a call for transparency of how such systems reach 
                decisions  has  echoed  within  academic  and  policy  circles.  The  term  transparency,  
                however,  relates  to  multiple  concepts,  fulfills  many  functions,  and  holds  different  
                promises that struggle to be realized in concrete applications. Indeed, the complex-
                ity  of  transparency  for  ADM  shows  tension  between  transparency  as  a  normative  
                ideal and its translation to practical application. To address this tension, we first con-
                duct  a  review  of  transparency,  analyzing  its  challenges  and  limitations  concerning  
                automated decision-making practices. We then look at the lessons learned from the 
                development  of  Privacy  by  Design,  as  a  basis  for  developing  the  Transparency  by  
                Design principles. Finally, we propose a set of nine principles to cover relevant con-
                textual, technical, informational, and stakeholder-sensitive considerations. Transpar-
                ency  by  Design  is  a  model  that  helps  organizations  design  transparent  AI  systems,  
                by integrating these principles in a step-by-step manner and as an ex-ante value, not 
                as an afterthought.
            \end{abstract}
        \end{itemize}
    \end{itemize}
    

\newpage
\section{Results}
In this section will be summarized the results of the analysis of the papers, with the goal to answer to the question about the possible approaches to create fair-by-Design mechanisms.

\subsection{What are the approaches used to make the AI trustworthy by design?}


\newpage
\section{Conclusion}
    % Summarize the main findings and conclusions go here...

\newpage
\section{References}
    % List of cited studies and sources go here...

\end{document}